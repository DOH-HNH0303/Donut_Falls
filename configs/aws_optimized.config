/*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    AWS-Specific Cost Optimization for Donut Falls
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    Optimized for AWS Batch with Seqera Platform using spot instances
----------------------------------------------------------------------------------------
*/

// Include base cost optimizations
includeConfig 'cost_optimized.config'

// AWS-specific configurations
aws {
    batch {
        // Enable spot instances with up to 50% discount
        spotPrice = '50%'
        
        // Optimize job queue settings
        maxParallelJobs = 100
        maxTransferAttempts = 3
        delayBetweenAttempts = '30 sec'
    }
    
    // Use cost-effective region (adjust as needed)
    region = 'us-west-2'
    
    client {
        // Optimize API calls
        maxConnections = 10
        maxErrorRetry = 3
    }
}

// Enable spot instances and cost optimization features
cloud {
    preemptible = true
    spot = true
}

// AWS instance type optimizations
process {
    
    // // Use compute-optimized instances for CPU-intensive processes
    // withName:unicycler {
    //     cpus   = { 8 }
    //     memory = { 32.GB }
    //     //time   = { 4.h }
    //     machineType = 'c5.2xlarge'  // 8 vCPUs, 16 GB RAM - cost-effective for compute
    //     disk = '100 GB'
    //     ext.args = '--keep 0'
    //     //ext.args = '--mode conservative --min_fasta_length 200'
    // }
    withName:unicycler {
        cpus = { 4 }
        memory = { 8.GB * task.attempt }    // Scales up on retry if needed
        machineType = 'c6g.xlarge'          // ARM-based, 60% cheaper
        disk = '50 GB'
        time = { 2.h * task.attempt }
        ext {
            args = '--keep 0 --mode conservative'
        }
        errorStrategy = { task.attempt < 3 ? 'retry' : 'terminate' }
        maxRetries = 2
}
    
    withName:flye {
        machineType = 'c5.4xlarge'  // 16 vCPUs, 32 GB - for larger assemblies
        disk = '200 GB'
        cpus   = { 16     }
    }
    
    withName:raven {
        machineType = 'c5.2xlarge'  // 8 vCPUs, 16 GB
        disk = '100 GB'
        cpus   = { 8     }
    }
    
    // Use general purpose instances for I/O intensive processes
    withName:circulocov {
        machineType = 'm5.xlarge'   // 4 vCPUs, 16 GB - balanced compute/memory
        disk = '100 GB'
        cpus   = { 4     }
    }
    
    withName:busco {
        machineType = 'm5.xlarge'   // 4 vCPUs, 16 GB
        disk = '50 GB'
        cpus   = { 4     }
    }
    
    // Use smaller instances for lightweight processes
    withName:fastp {
        machineType = 'm5.large'    // 2 vCPUs, 8 GB - $0.096/hr on-demand
        disk = '50 GB'
    }
    
    withName:seqkit {
        machineType = 't3.medium'   // 2 vCPUs, 4 GB - burstable, very cost-effective
        disk = '20 GB'
        cpus   = { 2     }
    }
    
    withName:mash {
        machineType = 't3.medium'   // 2 vCPUs, 4 GB
        disk = '20 GB'
        cpus   = { 2     }
    }
    
    withName:bandage {
        machineType = 't3.small'    // 2 vCPUs, 2 GB - minimal resources needed
        disk = '20 GB'
    }
    
    withName:gfastats {
        machineType = 't3.medium'   // 2 vCPUs, 4 GB
        disk = '20 GB'
        cpus   = { 2     }
    }
    
    // Memory-optimized for processes that need more RAM
    withName:clair3 {
        machineType = 'r5.large'    // 2 vCPUs, 16 GB - when memory is critical
        disk = '50 GB'
        cpus   = { 2     }
    }

    // WAPHL analysis processes
    withName:MASH_TAXA {
        machineType = 't3.large'    // 2 vCPUs, 8 GB - required for RefSeq database operations
        disk = '20 GB'
        cpus   = { 2 }
        memory = { 8.GB }  // Minimum 8GB required for RefSeq database operations
        //time   = { 15.m }
    }

    withName:FINAL_SUMMARY {
        machineType = 't3.small'    // 2 vCPUs, 2 GB - minimal resources for Python processing
        disk = '20 GB'
        cpus   = { 1 }
        memory = { 2.GB }
        //time   = { 10.m }
    }
}

// Storage optimizations
// workDir = 's3://your-bucket/work'  // Replace with your S3 bucket