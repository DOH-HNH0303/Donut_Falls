/*
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    AWS-Specific Cost Optimization for Donut Falls
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    Optimized for AWS Batch with Seqera Platform using spot instances
----------------------------------------------------------------------------------------
*/

// Include base cost optimizations
includeConfig 'cost_optimized.config'

// AWS-specific configurations
aws {
    batch {
        // Enable spot instances with up to 50% discount
        spotPrice = '60%'
        
        // Optimize job queue settings
        maxParallelJobs = 100
        maxTransferAttempts = 3
        delayBetweenAttempts = '30 sec'
    }
    
    // Use cost-effective region (adjust as needed)
    region = 'us-west-2'
    
    client {
        // Optimize API calls
        maxConnections = 10
        maxErrorRetry = 3
    }
}

// Enable spot instances and cost optimization features
cloud {
    preemptible = true
    spot = true
}

// AWS instance type optimizations
process {
    
    // // Use compute-optimized instances for CPU-intensive processes
    withName:unicycler {
        cpus   = { 8 }
        memory = { 64.GB }
        //time   = { 4.h }
        //machineType = 'r5.2xlarge'  // 8 vCPUs, 16 GB RAM - cost-effective for compute
        disk = '100 GB'
        ext.args = '--keep 0'
        errorStrategy = { task.attempt < 3 ? 'retry' : 'terminate' }
        maxRetries = 2
        //time = 4
        //ext.args = '--mode conservative --min_fasta_length 200'
    }
//     withName:unicycler {
//         cpus = { 4 }
//         memory = { 8.GB * task.attempt }    // Scales up on retry if needed
//         machineType = 'c6g.xlarge'          // ARM-based, 60% cheaper
//         disk = '50 GB'
//         time = { 2.h * task.attempt }
//         ext {
//             args = '--keep 0 --mode conservative'
//         }
//         errorStrategy = { task.attempt < 3 ? 'retry' : 'terminate' }
//         maxRetries = 2
// }
    
    withName:flye {
        //machineType = 'c5.4xlarge'  // 16 vCPUs, 32 GB - for larger assemblies
        disk = '200 GB'
        cpus   = { 16     }
        memory = { 32.GB     }
    }
    
    withName:raven {
        //machineType = 'c5.2xlarge'  // 8 vCPUs, 16 GB
        disk = '100 GB'
        cpus   = { 8     }
    }
    
    // Use general purpose instances for I/O intensive processes
    withName:circulocov {
        //machineType = 'm5.xlarge'   // 4 vCPUs, 16 GB - balanced compute/memory
        disk = '100 GB'
        cpus   = { 6     }
        memory   = { 8.GB     }
    }
    
    withName:busco {
        //machineType = 'm5.xlarge'   // 4 vCPUs, 16 GB
        disk = '50 GB'
        cpus   = { 4     }
    }
    
    // Use smaller instances for lightweight processes
    withName:fastp {
        //machineType = 'm5.large'    // 2 vCPUs, 8 GB - $0.096/hr on-demand
        disk = '50 GB'
    }
    
    withName:seqkit {
        //machineType = 't3.medium'   // 2 vCPUs, 4 GB - burstable, very cost-effective
        disk = '20 GB'
        cpus   = { 2     }
    }
    
    withName:mash {
        //machineType = 't3.medium'   // 2 vCPUs, 4 GB
        disk = '20 GB'
        cpus   = { 2     }
    }
    
    withName:bandage {
        //machineType = 't3.small'    // 2 vCPUs, 2 GB - minimal resources needed
        disk = '20 GB'
    }
    
    withName:gfastats {
        //machineType = 't3.medium'   // 2 vCPUs, 4 GB
        disk = '20 GB'
        cpus   = { 2     }
    }
    
    // Memory-optimized for processes that need more RAM
    withName:clair3 {
        //machineType = 'r5.large'    // 2 vCPUs, 16 GB - when memory is critical
        disk = '50 GB'
        cpus   = { 4     }
        memory = { 16.GB }
    }

        // ONT Coverage analysis - requires mapping and depth calculation
    withName:ONT_COVERAGE {
        //machineType = 'c5.xlarge'   // 4 vCPUs, 8 GB - compute-optimized for minimap2 mapping
        disk = '100 GB'             // Sufficient space for BAM files and intermediate files
        cpus   = { 4 }
        memory = { 8.GB }
        time   = { 30.m }
        errorStrategy = { task.attempt < 2 ? 'retry' : 'terminate' }
        maxRetries = 1
    }

    // WAPHL analysis processes
    withName:MASH_TAXA {
        //machineType = 't3.large'    // 2 vCPUs, 8 GB - required for RefSeq database operations
        disk = '20 GB'
        cpus   = { 2 }
        memory = { 8.GB }  // Minimum 8GB required for RefSeq database operations
        //time   = { 15.m }
    }

    withName:FINAL_SUMMARY {
        //machineType = 't3.small'    // 2 vCPUs, 2 GB - minimal resources for Python processing
        disk = '20 GB'
        cpus   = { 1 }
        memory = { 2.GB }
        //time   = { 10.m }
    }

    // Human contamination detection - similar to MASH_TAXA but focused on human sequences
    withName:MASH_HUMAN_CONTAMINATION {
        //machineType = 't3.large'    // 2 vCPUs, 8 GB - required for RefSeq database operations
        disk = '20 GB'
        cpus   = { 2 }
        memory = { 8.GB }  // Minimum 8GB required for RefSeq database operations
        //time   = { 15.m }
        errorStrategy = { task.attempt < 2 ? 'retry' : 'terminate' }
        maxRetries = 1
    }

    // Coverage analysis - requires mapping with BWA/minimap2 and depth calculation
    withName:COVERAGE_ANALYSIS {
        //machineType = 'c5.xlarge'   // 4 vCPUs, 8 GB - compute-optimized for mapping operations
        disk = '100 GB'             // Sufficient space for BAM files and intermediate files
        cpus   = { 4 }
        memory = { 8.GB }
        time   = { 30.m }
        errorStrategy = { task.attempt < 2 ? 'retry' : 'terminate' }
        maxRetries = 1
    }

    withName:HOSTILE {
        cpus = { 8 }
        memory = { 16.GB }
    }

    withName:summary {
        cpus = { 1 }
        memory = { 2.GB }
    }

    withName:FINAL_SUMMARY {
        cpus = { 1 }
        memory = { 2.GB }
    }

    withName:versions {
        cpus = { 1 }
        memory = { 2.GB }
    }
}

// Storage optimizations
// workDir = 's3://your-bucket/work'  // Replace with your S3 bucket